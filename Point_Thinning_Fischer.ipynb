{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from simpledbf import Dbf5\n",
    "import os\n",
    "import pyproj\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_pyth(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Calculate the distance between two points \"\"\"\n",
    "    return np.sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2)\n",
    "\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between one point \n",
    "    on the earth and an array of points (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371000 # Radius of earth in meters\n",
    "    return c * r\n",
    "\n",
    "\n",
    "def p_dist(lon1, lat1, lon2, lat2, units=None):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two *CLOSE* points in meters using Pythagoras\n",
    "    \"\"\"\n",
    "    if units == 'm':\n",
    "        dist = dist_pyth(lon1, lat1, lon2, lat2)\n",
    "    elif units == 'deg':\n",
    "        dist = haversine(lon1, lat1, lon2, lat2)\n",
    "    else:\n",
    "        raise ValueError('Units must be meters (m) or degrees (deg).')\n",
    "        \n",
    "    return dist\n",
    "\n",
    "\n",
    "def p_thin(df, xcol='x', ycol='y', datacols='data', radius=10, method='nanmean', units=None):\n",
    "    \"\"\"\n",
    "    Thin a pandas point series based on distance of the points. The first point in the DataFrame is taken and all points \n",
    "    within the search distance are found and the given method is applied to the data columns. Used data are deleted from \n",
    "    the frame and the procedure is repeated. Data are written to the point which is closest to the half of the search\n",
    "    distance away from the initial point.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pseries: Series of points\n",
    "    xcol: name of column with x coordinates\n",
    "    ycol: name of columns with y coordinates\n",
    "    datacol: name of columns with data (list/string)\n",
    "    radius: search radius for point distance (meters)\n",
    "    method: calculation method (a numpy method). Default: 'nanmean'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A thinned dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert units!=None,'Units of x/y must be meters (m) or degrees (deg).'\n",
    "    \n",
    "    thinned = pd.DataFrame([],columns=df.columns.values)\n",
    "    \n",
    "    print(len(df))\n",
    "    ct=0\n",
    "    while len(df)>0:\n",
    "        df['DIST'] = p_dist(df[xcol].iloc[0], df[ycol].iloc[0], df[xcol], df[ycol], units=units)\n",
    "        df = df.sort_values('DIST')\n",
    "\n",
    "        subset = df[df.DIST <= radius]\n",
    "\n",
    "        if not subset.empty:\n",
    "            subset = subset.reset_index()\n",
    "            # find the point ID which is closest to half of the search distance \n",
    "            #(to this point the new data will be assigned)\n",
    "            assign_id = subset.ix[(subset.DIST-(radius/2.)).abs().argsort()[0]].name\n",
    "            \n",
    "            fill_dict = {}\n",
    "            fill_dict[xcol] = subset.loc[assign_id, xcol]\n",
    "            fill_dict[ycol] = subset.loc[assign_id, ycol]\n",
    "            for data in datacols:\n",
    "                try:\n",
    "                    fill_dict[data] = getattr(np,method)(subset[data].values)\n",
    "                except TypeError:\n",
    "                    fill_dict[data] = np.nan\n",
    "            thinned.loc[len(thinned)] = pd.Series(fill_dict)\n",
    "            ct+=1\n",
    "\n",
    "            if ct%100==0:\n",
    "                print(ct,len(df))\n",
    "                \n",
    "        # delete the used points from the data\n",
    "        df = df.drop(df[df.DIST <= radius].index.values)\n",
    "        \n",
    "    print(len(thinned))\n",
    "    return thinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = glob('C:\\\\Users\\\\jlandman\\\\Desktop\\\\newData\\\\AT-IT-KayHelfricht\\\\Fischer-etal-2015\\\\*ferner*\\\\*.dbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop_list = ['Gliederferner', 'Langenferner', 'Rieserferner', 'Uebeltalferner', 'Weissbrunnferner', 'Weissbrunnferner1995']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ed39e14f96af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpop_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m             \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "for i in files:\n",
    "    for j in pop_list:\n",
    "        if j in i:\n",
    "            files.remove(i)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thin_dict = {'Feuersteinferner': 25,\n",
    "             'Grafferner': 15,\n",
    "             'Hangendenferner': 25,\n",
    "             'ed_Hohenferner':25,\n",
    "             'Laaserferner':65,\n",
    "             'Langtaufererferner':20,\n",
    "             'Matscherferner':20,\n",
    "             'obererOrtlerferner':2,\n",
    "             'Schranferner':10,\n",
    "             'Suldenferner':20\n",
    "    \n",
    "}\n",
    "\n",
    "date_dict = {'Feuersteinferner': 20130608,\n",
    "             'Grafferner': 20130612,\n",
    "             'Hangendenferner': 20130608,\n",
    "             'ed_Hohenferner':20130514,\n",
    "             'Laaserferner':20130513,\n",
    "             'Langtaufererferner':20130613,\n",
    "             'Matscherferner':20130612,\n",
    "             'obererOrtlerferner':np.nan,\n",
    "             'Schranferner':20130514,\n",
    "             'Suldenferner':20130611}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_dist = 10  # forward search distance for thinning \n",
    "\n",
    "# Some constants\n",
    "gtd_id = np.nan # GlaThiDa_ID\n",
    "punit = 'IT' # political unit\n",
    "gname = ''\n",
    "pid = np.nan\n",
    "thickness_unc = np.nan\n",
    "data_flag = np.nan\n",
    "remarks = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utm32 = pyproj.Proj(init='epsg:32632') #WGS84 UTM32N\n",
    "latlon = pyproj.Proj(init='epsg:4326')  #WGS84 lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEUERSTEINFERNER\n",
      "25287\n",
      "100 21411\n",
      "200 15322\n",
      "300 11735\n",
      "400 7667\n",
      "500 2603\n",
      "590\n",
      "GRAFFERNER\n",
      "13608\n",
      "100 10544\n",
      "200 7860\n",
      "300 5063\n",
      "400 3526\n",
      "491\n",
      "HANGENDENFERNER\n",
      "23549\n",
      "100 17558\n",
      "200 13315\n",
      "300 8714\n",
      "400 5532\n",
      "494\n",
      "ED_HOHENFERNER\n",
      "27486\n",
      "100 23511\n",
      "200 19624\n",
      "300 17340\n",
      "400 12510\n",
      "500 6882\n",
      "600 1624\n",
      "631\n",
      "LAASERFERNER\n",
      "65895\n",
      "100 54706\n",
      "200 49549\n",
      "300 43466\n",
      "400 37018\n",
      "500 32127\n",
      "600 28036\n",
      "700 18232\n",
      "800 9728\n",
      "891\n",
      "LANGTAUFERERFERNER\n",
      "20909\n",
      "100 18738\n",
      "200 17155\n",
      "300 15386\n",
      "400 13212\n",
      "500 9932\n",
      "600 4286\n",
      "700 2280\n",
      "800 425\n",
      "830\n",
      "MATSCHERFERNER\n",
      "20392\n",
      "100 16464\n",
      "200 12221\n",
      "300 8561\n",
      "400 6440\n",
      "500 4505\n",
      "600 1034\n",
      "641\n",
      "Table obererOrtlerferner.dbf has no coordinates\n",
      "SCHRANFERNER\n",
      "9275\n",
      "100 6273\n",
      "200 3763\n",
      "300 1040\n",
      "315\n",
      "SULDENFERNER\n",
      "19358\n",
      "100 13000\n",
      "200 7718\n",
      "300 5020\n",
      "400 1030\n",
      "431\n",
      "Table Weissbrunn1995.dbf has no coordinates\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    fname = os.path.basename(file)\n",
    "    dbf = Dbf5(file)\n",
    "    data = dbf.to_dataframe()\n",
    "    data.columns = map(str.lower, data.columns) # make all columns lowercase, that's easier\n",
    "    #former data thinning\n",
    "    #data = data.groupby(data.index // thin_dict[fname.split('.')[0]]).mean() # average over every x values as specified above\n",
    "    \n",
    "    try:\n",
    "        # project UTM values to lat/lon\n",
    "        xs = data.x.values\n",
    "        ys = data.y.values\n",
    "        \n",
    "        x1,y1 = utm32(xs, ys)\n",
    "        lons, lats = pyproj.transform(utm32,latlon,xs,ys)\n",
    "        \n",
    "        data['POINT_LAT'] = lats\n",
    "        data['POINT_LON'] = lons\n",
    "        \n",
    "        try:\n",
    "            data['ELEVATION'] = (data['ed'] + data['ug']).round()\n",
    "        except KeyError:\n",
    "            data['ELEVATION'] = np.nan\n",
    "            \n",
    "        data['THICKNESS'] = data['ed'].round()\n",
    "        data['THICKNESS_UNCERTAINTY'] = thickness_unc\n",
    "        \n",
    "        # time to thin the data points\n",
    "        print(fname.split('.')[0].upper())\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            data = p_thin(data, xcol='POINT_LON', ycol='POINT_LAT', datacols=['ELEVATION', 'THICKNESS', 'THICKNESS_UNCERTAINTY'], radius=search_dist, method='nanmean', units='deg')\n",
    "        \n",
    "        data['ELEVATION'] = data['ELEVATION'].round()\n",
    "        data['THICKNESS'] = data['THICKNESS'].round()\n",
    "        \n",
    "        data['GlaThiDa_ID'] = gtd_id\n",
    "        data['POLITICAL_UNIT'] = punit\n",
    "        data['GLACIER_NAME'] = fname.split('.')[0].upper()\n",
    "        data['SURVEY_DATE'] = date_dict[fname.split('.')[0]]\n",
    "        data['POINT_ID'] = range(1,len(data)+1)\n",
    "        data['DATA_FLAG'] = data_flag\n",
    "        data['REMARKS'] = remarks + ' Point data have been thinned (mean) within a search distance of %s m.' % search_dist\n",
    "        \n",
    "        data = data[['GlaThiDa_ID','POLITICAL_UNIT','GLACIER_NAME','SURVEY_DATE','POINT_ID','POINT_LAT','POINT_LON',\n",
    "                     'ELEVATION','THICKNESS','THICKNESS_UNCERTAINTY','DATA_FLAG','REMARKS']]\n",
    "        data.to_excel(os.path.join(os.path.dirname(file),fname.split('.')[0]+'.xlsx'), index=False)\n",
    "    except AttributeError:\n",
    "        print('Table %s has no coordinates' % fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
